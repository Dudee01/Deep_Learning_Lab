
Experiment 1:

Model Architecture Hyperparameters:
Input shape: (32, 32, 3) - CIFAR-10 image dimensions
Hidden layer sizes:
First hidden layer: 1024 neurons
Second hidden layer: 512 neurons
Third hidden layer: 256 neurons
Fourth hidden layer: 128 neurons
Output layer size: 10 neurons (for 10 CIFAR-10 classes)

Activation functions:
Hidden layers: 'relu'
Output layer: 'softmax'

Training Hyperparameters:
Learning rate: 0.0001 (set in Adam optimizer)
Optimizer: Adam
Loss function: 'categorical_crossentropy'
Number of epochs: 50
Batch size: 64
Validation split: 0.2 (20% of training data used for validation)




PS C:\Users\abhis\Desktop\4YS1\DL\DL_LABS> & C:/Users/abhis/Desktop/4YS1/DL/DL_LABS/venv/Scripts/Activate.ps1
(venv) PS C:\Users\abhis\Desktop\4YS1\DL\DL_LABS> & C:/Users/abhis/Desktop/4YS1/DL/DL_LABS/venv/Scripts/python.exe "c:/Users/abhis/Desktop/4YS1/DL/DL_LABS/week(01-08-25)/cifar_mlp_keras.py"
2025-08-07 21:32:51.454053: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-08-07 21:32:54.639543: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz
170498071/170498071 ━━━━━━━━━━━━━━━━━━━━ 27s 0us/step     
C:\Users\abhis\Desktop\4YS1\DL\DL_LABS\venv\Lib\site-packages\keras\src\layers\reshaping\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
2025-08-07 21:33:29.567732: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Epoch 1/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 20s 30ms/step - accuracy: 0.2131 - loss: 16.0810 - val_accuracy: 0.2716 - val_loss: 4.3749
Epoch 2/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 19s 31ms/step - accuracy: 0.2943 - loss: 3.4063 - val_accuracy: 0.3206 - val_loss: 2.6788
Epoch 3/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 21s 31ms/step - accuracy: 0.3361 - loss: 2.4372 - val_accuracy: 0.3271 - val_loss: 2.3414
Epoch 4/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 20s 32ms/step - accuracy: 0.3637 - loss: 2.0979 - val_accuracy: 0.3585 - val_loss: 2.0156
Epoch 5/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 22s 35ms/step - accuracy: 0.3848 - loss: 1.9137 - val_accuracy: 0.3701 - val_loss: 2.0282
Epoch 6/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 20s 32ms/step - accuracy: 0.4187 - loss: 1.7387 - val_accuracy: 0.3740 - val_loss: 1.9099
Epoch 7/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 27s 43ms/step - accuracy: 0.4304 - loss: 1.6830 - val_accuracy: 0.3878 - val_loss: 1.8465
Epoch 8/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 28s 45ms/step - accuracy: 0.4313 - loss: 1.6461 - val_accuracy: 0.4026 - val_loss: 1.7771
Epoch 9/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 24s 38ms/step - accuracy: 0.4509 - loss: 1.5761 - val_accuracy: 0.4305 - val_loss: 1.6934
Epoch 10/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 25s 40ms/step - accuracy: 0.4599 - loss: 1.5406 - val_accuracy: 0.4100 - val_loss: 1.7664
Epoch 11/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 22s 35ms/step - accuracy: 0.4589 - loss: 1.5330 - val_accuracy: 0.4214 - val_loss: 1.6857
Epoch 12/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 22s 34ms/step - accuracy: 0.4762 - loss: 1.4909 - val_accuracy: 0.4095 - val_loss: 1.8125
Epoch 13/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 21s 33ms/step - accuracy: 0.4679 - loss: 1.4977 - val_accuracy: 0.4440 - val_loss: 1.6051
Epoch 14/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 21s 33ms/step - accuracy: 0.4801 - loss: 1.4674 - val_accuracy: 0.4421 - val_loss: 1.6412
Epoch 15/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 22s 36ms/step - accuracy: 0.4977 - loss: 1.4222 - val_accuracy: 0.4489 - val_loss: 1.5929
Epoch 16/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 24s 38ms/step - accuracy: 0.5076 - loss: 1.4010 - val_accuracy: 0.4534 - val_loss: 1.5821
Epoch 17/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 25s 39ms/step - accuracy: 0.5072 - loss: 1.3787 - val_accuracy: 0.4448 - val_loss: 1.6381
Epoch 18/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 21s 34ms/step - accuracy: 0.5133 - loss: 1.3669 - val_accuracy: 0.4578 - val_loss: 1.5846
Epoch 19/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 21s 34ms/step - accuracy: 0.5352 - loss: 1.3194 - val_accuracy: 0.4545 - val_loss: 1.6227
Epoch 20/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 21s 34ms/step - accuracy: 0.5306 - loss: 1.3139 - val_accuracy: 0.4559 - val_loss: 1.6037
Epoch 21/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 21s 34ms/step - accuracy: 0.5430 - loss: 1.2743 - val_accuracy: 0.4671 - val_loss: 1.6017
Epoch 22/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 23s 37ms/step - accuracy: 0.5437 - loss: 1.2759 - val_accuracy: 0.4706 - val_loss: 1.5532
Epoch 23/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 23s 36ms/step - accuracy: 0.5606 - loss: 1.2287 - val_accuracy: 0.4852 - val_loss: 1.5372
Epoch 24/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 24s 39ms/step - accuracy: 0.5778 - loss: 1.1902 - val_accuracy: 0.4670 - val_loss: 1.5878
Epoch 25/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 23s 37ms/step - accuracy: 0.5798 - loss: 1.1797 - val_accuracy: 0.4753 - val_loss: 1.5830
Epoch 26/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 25s 39ms/step - accuracy: 0.5850 - loss: 1.1628 - val_accuracy: 0.4802 - val_loss: 1.6022
Epoch 27/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 25s 39ms/step - accuracy: 0.6008 - loss: 1.1235 - val_accuracy: 0.4867 - val_loss: 1.5454
Epoch 28/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 24s 38ms/step - accuracy: 0.6087 - loss: 1.0944 - val_accuracy: 0.4875 - val_loss: 1.5719
Epoch 29/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 25s 39ms/step - accuracy: 0.6179 - loss: 1.0737 - val_accuracy: 0.5014 - val_loss: 1.5624
Epoch 30/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 32s 51ms/step - accuracy: 0.6329 - loss: 1.0342 - val_accuracy: 0.4973 - val_loss: 1.5943
Epoch 31/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 30s 47ms/step - accuracy: 0.6381 - loss: 1.0166 - val_accuracy: 0.4806 - val_loss: 1.6738
Epoch 32/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 29s 47ms/step - accuracy: 0.6429 - loss: 0.9993 - val_accuracy: 0.4796 - val_loss: 1.6736
Epoch 33/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 24s 39ms/step - accuracy: 0.6563 - loss: 0.9682 - val_accuracy: 0.4846 - val_loss: 1.7243
Epoch 34/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 31s 50ms/step - accuracy: 0.6641 - loss: 0.9462 - val_accuracy: 0.4796 - val_loss: 1.7305
Epoch 35/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 29s 47ms/step - accuracy: 0.6689 - loss: 0.9266 - val_accuracy: 0.4881 - val_loss: 1.7015
Epoch 36/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 30s 47ms/step - accuracy: 0.6792 - loss: 0.8954 - val_accuracy: 0.4878 - val_loss: 1.7233
Epoch 37/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 30s 48ms/step - accuracy: 0.6893 - loss: 0.8767 - val_accuracy: 0.4977 - val_loss: 1.7379
Epoch 38/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 26s 41ms/step - accuracy: 0.6890 - loss: 0.8617 - val_accuracy: 0.4984 - val_loss: 1.7580
Epoch 39/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 26s 42ms/step - accuracy: 0.7000 - loss: 0.8416 - val_accuracy: 0.4830 - val_loss: 1.8734
Epoch 40/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 25s 41ms/step - accuracy: 0.7079 - loss: 0.8173 - val_accuracy: 0.4927 - val_loss: 1.8012
Epoch 41/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 26s 41ms/step - accuracy: 0.7192 - loss: 0.7809 - val_accuracy: 0.5082 - val_loss: 1.7646
Epoch 42/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 25s 41ms/step - accuracy: 0.7279 - loss: 0.7614 - val_accuracy: 0.4972 - val_loss: 1.8414
Epoch 43/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 25s 39ms/step - accuracy: 0.7377 - loss: 0.7276 - val_accuracy: 0.4898 - val_loss: 1.8820
Epoch 44/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 24s 39ms/step - accuracy: 0.7407 - loss: 0.7325 - val_accuracy: 0.4899 - val_loss: 1.9280
Epoch 45/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 24s 38ms/step - accuracy: 0.7521 - loss: 0.7015 - val_accuracy: 0.4975 - val_loss: 1.9280
Epoch 46/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 24s 38ms/step - accuracy: 0.7595 - loss: 0.6861 - val_accuracy: 0.4953 - val_loss: 1.9918
Epoch 47/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 23s 38ms/step - accuracy: 0.7612 - loss: 0.6666 - val_accuracy: 0.5051 - val_loss: 2.0062
Epoch 48/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 24s 38ms/step - accuracy: 0.7696 - loss: 0.6555 - val_accuracy: 0.5046 - val_loss: 1.9800
Epoch 49/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 25s 40ms/step - accuracy: 0.7755 - loss: 0.6340 - val_accuracy: 0.4985 - val_loss: 2.0586
Epoch 50/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 23s 37ms/step - accuracy: 0.7873 - loss: 0.6047 - val_accuracy: 0.5015 - val_loss: 2.1070
313/313 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - accuracy: 0.4990 - loss: 2.0924  
0.5015000104904175
313/313 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step  
predicted label:8


Experiment 2:

Model Architecture Hyperparameters:
Convolutional Layer 1: 32 filters, (3,3) kernel size, 'relu' activation
MaxPooling Layer 1: (2,2) pool size
Convolutional Layer 2: 32 filters, (3,3) kernel size, 'relu' activation
MaxPooling Layer 2: (2,2) pool size

Dense Layer sizes:
First hidden layer: 1024 neurons
Second hidden layer: 512 neurons  
Third hidden layer: 256 neurons
Fourth hidden layer: 128 neurons
Fifth hidden layer: 64 neurons
Output layer size: 10 neurons (for 10 CIFAR-10 classes)

Activation functions:
Convolutional layers: 'relu'
Hidden layers: 'relu'
Output layer: 'softmax'

Training Hyperparameters:
Learning rate: 0.0001 (set in Adam optimizer)
Optimizer: Adam
Loss function: 'categorical_crossentropy'
Number of epochs: 50
Batch size: 64
Validation split: 0.2 (20% of training data used for validation)

Callback Configuration:
ModelCheckpoint: Saves best model based on validation loss ('val_loss')
Monitoring mode: 'min' (saves when validation loss decreases)
Save best only: True

The key difference from Experiment 1 is that Experiment 2 uses Convolutional
 Neural Network (CNN) architecture with Conv2D and MaxPooling2D layers before 
 the dense layers, which is better suited for image classification tasks like CIFAR-10.


 (venv) PS C:\Users\abhis\Desktop\4YS1\DL\DL_LABS> & C:/Users/abhis/Desktop/4YS1/DL/DL_LABS/venv/Scripts/python.exe "c:/Users/abhis/Desktop/4YS1/DL/DL_LABS/week(01-08-25)/P7.py"
2025-08-07 21:59:55.102560: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
bhis/Desktop/4YS1/DL/DL_LABS/week(01-08-25)/P7.py"
2025-08-07 21:59:55.102560: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
t numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-08-07 21:59:56.005519: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-08-07 21:59:56.005519: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
t numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-08-07 21:59:59.648921: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-08-07 21:59:59.648921: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Epoch 1/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - accuracy: 0.2958 - loss: 2.7385WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead ailable CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Flow with the appropriate compiler flags.
Epoch 1/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - accuracy: 0.2958 - loss: 2.7385WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`.
625/625 ━━━━━━━━━━━━━━━━━━━━ 17s 24ms/step - accuracy: 0.2959 - loss: 2.7372 - val_accuracy: 0.4826 - val_loss: 1.4782        
Epoch 2/50
624/625 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 0.5086 - loss: 1.3863WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`.
625/625 ━━━━━━━━━━━━━━━━━━━━ 15s 25ms/step - accuracy: 0.5086 - loss: 1.3862 - val_accuracy: 0.5110 - val_loss: 1.3995
Epoch 3/50
624/625 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 0.5838 - loss: 1.1860WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`.
625/625 ━━━━━━━━━━━━━━━━━━━━ 17s 27ms/step - accuracy: 0.5839 - loss: 1.1859 - val_accuracy: 0.5664 - val_loss: 1.2317
Epoch 4/50
623/625 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 0.6332 - loss: 1.0426WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`.
625/625 ━━━━━━━━━━━━━━━━━━━━ 16s 25ms/step - accuracy: 0.6332 - loss: 1.0426 - val_accuracy: 0.6074 - val_loss: 1.1469
Epoch 5/50
623/625 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 0.6821 - loss: 0.9147WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`.
625/625 ━━━━━━━━━━━━━━━━━━━━ 15s 24ms/step - accuracy: 0.6821 - loss: 0.9147 - val_accuracy: 0.6117 - val_loss: 1.1422
Epoch 6/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 15s 24ms/step - accuracy: 0.7263 - loss: 0.7940 - val_accuracy: 0.5996 - val_loss: 1.1863
Epoch 7/50
624/625 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 0.7655 - loss: 0.6815WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`.
625/625 ━━━━━━━━━━━━━━━━━━━━ 15s 25ms/step - accuracy: 0.7655 - loss: 0.6815 - val_accuracy: 0.6347 - val_loss: 1.1141
Epoch 8/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 16s 25ms/step - accuracy: 0.8122 - loss: 0.5516 - val_accuracy: 0.6228 - val_loss: 1.1890
Epoch 9/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 15s 25ms/step - accuracy: 0.8551 - loss: 0.4350 - val_accuracy: 0.6263 - val_loss: 1.2510
Epoch 10/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 15s 24ms/step - accuracy: 0.8973 - loss: 0.3220 - val_accuracy: 0.6318 - val_loss: 1.2952
Epoch 11/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 15s 24ms/step - accuracy: 0.9230 - loss: 0.2421 - val_accuracy: 0.6412 - val_loss: 1.3785
Epoch 12/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 15s 24ms/step - accuracy: 0.9489 - loss: 0.1652 - val_accuracy: 0.6395 - val_loss: 1.4727
Epoch 13/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 16s 25ms/step - accuracy: 0.9616 - loss: 0.1324 - val_accuracy: 0.6302 - val_loss: 1.6160
Epoch 14/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 15s 25ms/step - accuracy: 0.9712 - loss: 0.0994 - val_accuracy: 0.6211 - val_loss: 1.8671
Epoch 15/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 15s 25ms/step - accuracy: 0.9737 - loss: 0.0887 - val_accuracy: 0.6226 - val_loss: 1.8356
Epoch 16/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 15s 25ms/step - accuracy: 0.9781 - loss: 0.0740 - val_accuracy: 0.6170 - val_loss: 1.9441
Epoch 17/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 15s 25ms/step - accuracy: 0.9800 - loss: 0.0653 - val_accuracy: 0.6361 - val_loss: 1.9727
Epoch 18/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 16s 25ms/step - accuracy: 0.9798 - loss: 0.0672 - val_accuracy: 0.6144 - val_loss: 2.1130
Epoch 19/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 16s 25ms/step - accuracy: 0.9805 - loss: 0.0620 - val_accuracy: 0.6437 - val_loss: 2.0502
Epoch 20/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 16s 26ms/step - accuracy: 0.9840 - loss: 0.0527 - val_accuracy: 0.6266 - val_loss: 2.2075
Epoch 21/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 20s 32ms/step - accuracy: 0.9834 - loss: 0.0534 - val_accuracy: 0.6162 - val_loss: 2.4149
Epoch 22/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 24s 38ms/step - accuracy: 0.9855 - loss: 0.0482 - val_accuracy: 0.6382 - val_loss: 2.0714
Epoch 23/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 24s 39ms/step - accuracy: 0.9885 - loss: 0.0377 - val_accuracy: 0.6274 - val_loss: 2.3061
Epoch 24/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 17s 27ms/step - accuracy: 0.9845 - loss: 0.0505 - val_accuracy: 0.6379 - val_loss: 2.1232
Epoch 25/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 17s 27ms/step - accuracy: 0.9909 - loss: 0.0306 - val_accuracy: 0.6409 - val_loss: 2.2962
Epoch 26/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 17s 27ms/step - accuracy: 0.9815 - loss: 0.0562 - val_accuracy: 0.6399 - val_loss: 2.3127
Epoch 27/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 19s 30ms/step - accuracy: 0.9878 - loss: 0.0392 - val_accuracy: 0.6461 - val_loss: 2.3537
Epoch 28/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 18s 29ms/step - accuracy: 0.9887 - loss: 0.0358 - val_accuracy: 0.6281 - val_loss: 2.4193
Epoch 29/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 16s 26ms/step - accuracy: 0.9880 - loss: 0.0383 - val_accuracy: 0.6258 - val_loss: 2.4314
Epoch 30/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 17s 27ms/step - accuracy: 0.9865 - loss: 0.0387 - val_accuracy: 0.6472 - val_loss: 2.3285
Epoch 31/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 17s 27ms/step - accuracy: 0.9923 - loss: 0.0240 - val_accuracy: 0.6391 - val_loss: 2.4523
Epoch 32/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 16s 26ms/step - accuracy: 0.9899 - loss: 0.0307 - val_accuracy: 0.6398 - val_loss: 2.3159
Epoch 33/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 17s 27ms/step - accuracy: 0.9904 - loss: 0.0320 - val_accuracy: 0.6346 - val_loss: 2.5499
Epoch 34/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 16s 26ms/step - accuracy: 0.9910 - loss: 0.0294 - val_accuracy: 0.6455 - val_loss: 2.4816
Epoch 35/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 16s 26ms/step - accuracy: 0.9859 - loss: 0.0429 - val_accuracy: 0.6434 - val_loss: 2.3323
Epoch 36/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 16s 26ms/step - accuracy: 0.9946 - loss: 0.0201 - val_accuracy: 0.6344 - val_loss: 2.6215
Epoch 37/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 17s 26ms/step - accuracy: 0.9895 - loss: 0.0334 - val_accuracy: 0.6343 - val_loss: 2.5426
Epoch 38/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 16s 26ms/step - accuracy: 0.9889 - loss: 0.0344 - val_accuracy: 0.6397 - val_loss: 2.4918
Epoch 39/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 17s 26ms/step - accuracy: 0.9958 - loss: 0.0142 - val_accuracy: 0.6421 - val_loss: 2.4952
Epoch 40/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 16s 26ms/step - accuracy: 0.9920 - loss: 0.0251 - val_accuracy: 0.6362 - val_loss: 2.5349
Epoch 41/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 16s 26ms/step - accuracy: 0.9909 - loss: 0.0282 - val_accuracy: 0.6373 - val_loss: 2.6024
Epoch 42/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 16s 26ms/step - accuracy: 0.9926 - loss: 0.0243 - val_accuracy: 0.6423 - val_loss: 2.5841
Epoch 43/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 17s 27ms/step - accuracy: 0.9911 - loss: 0.0276 - val_accuracy: 0.6460 - val_loss: 2.5965
Epoch 44/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 17s 27ms/step - accuracy: 0.9904 - loss: 0.0279 - val_accuracy: 0.6372 - val_loss: 2.5498
Epoch 45/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 17s 27ms/step - accuracy: 0.9919 - loss: 0.0230 - val_accuracy: 0.6496 - val_loss: 2.6200
Epoch 46/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 17s 28ms/step - accuracy: 0.9930 - loss: 0.0230 - val_accuracy: 0.6382 - val_loss: 2.6723
Epoch 47/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 16s 26ms/step - accuracy: 0.9930 - loss: 0.0209 - val_accuracy: 0.6459 - val_loss: 2.7182
Epoch 48/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 16s 26ms/step - accuracy: 0.9901 - loss: 0.0302 - val_accuracy: 0.6433 - val_loss: 2.6389
Epoch 49/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 17s 27ms/step - accuracy: 0.9942 - loss: 0.0187 - val_accuracy: 0.6304 - val_loss: 2.6620
Epoch 50/50
625/625 ━━━━━━━━━━━━━━━━━━━━ 16s 26ms/step - accuracy: 0.9931 - loss: 0.0203 - val_accuracy: 0.6359 - val_loss: 2.6646        
313/313 ━━━━━━━━━━━━━━━━━━━━ 2s 5ms/step - accuracy: 0.6311 - loss: 2.7598
Test accuracy: 0.6322000026702881
313/313 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step
Predicted label: 8


